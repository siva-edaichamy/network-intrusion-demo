# -*- coding: utf-8 -*-
"""kdd_intrusion_detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZMuaH1kjgzV849hTvfT_MY2BmcyFWJ6w

# KDD Intrusion Detection Classification Model

This notebook builds a classification model to predict network intrusion types using the KDD Cup 1999 dataset.

## Dataset Overview
- **Goal**: Classify network connections as normal or various types of attacks
- **Features**: 41 features (mix of continuous, discrete, and symbolic)
- **Target**: Attack type (normal, neptune, smurf, back, teardrop, etc.)

## 1. Import Libraries
"""

# Data manipulation and analysis
import pandas as pd
import numpy as np
import json
import os
import sys
from pathlib import Path

# Preprocessing
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score

# Classification models
from sklearn.ensemble import RandomForestClassifier

# Evaluation metrics
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    accuracy_score,
    precision_score,
    recall_score,
    f1_score
)

# Ignore warnings
import warnings
warnings.filterwarnings('ignore')

print("Libraries imported successfully!")
print(f"[STEP 1/6] Libraries loaded")
sys.stdout.flush()  # Ensure immediate output

"""## 2. Load and Explore the Data"""

# Define column names for KDD dataset
column_names = [
    'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes',
    'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in',
    'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations',
    'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login',
    'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate',
    'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate',
    'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count',
    'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate',
    'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate',
    'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'attack_type', 'difficulty_level'
]

# Load the dataset - use training dataset path
# Try to get dataset path from config, environment, or use default
dataset_path = None
try:
    # Add parent directory to path to import from src
    sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
    from src.config_manager import get_config
    config = get_config()
    dataset_path = config.get_training_dataset_path()
    # Check if it's a .txt file (KDD format) or .csv
    if not os.path.exists(dataset_path):
        # Try .txt version
        txt_path = dataset_path.replace('.csv', '.txt')
        if os.path.exists(txt_path):
            dataset_path = txt_path
except:
    # Fallback to default training dataset
    dataset_path = './data/kdd_cup_data_history.csv'

# If dataset path doesn't exist and we're not in Colab, try to find it
if not os.path.exists(dataset_path) and '/content' not in dataset_path:
    # Try common locations for training dataset
    possible_paths = [
        './data/kdd_cup_data_history.csv',
        './data/kdd_cup_data_history.txt',
        '../data/kdd_cup_data_history.csv',
        '../data/kdd_cup_data_history.txt',
        # Fallback to old name if history file doesn't exist
        './data/kdd_cup_data.csv',
        './data/KDDTrain+_20Percent.txt',
    ]
    for path in possible_paths:
        if os.path.exists(path):
            dataset_path = path
            break

if not os.path.exists(dataset_path):
    print(f"WARNING: Training dataset not found at {dataset_path}")
    print("Please set KDD_DATASET_TRAINING_PATH environment variable or place dataset in ./data/")
    print(f"Expected file: kdd_cup_data_history.csv (or .txt)")
    raise FileNotFoundError(f"Training dataset file not found: {dataset_path}")

print(f"[STEP 2/6] Loading dataset from: {dataset_path}")
sys.stdout.flush()

# Detect if file is CSV with headers or text file without headers
has_header = False
try:
    # Try reading first line to check for headers
    with open(dataset_path, 'r') as f:
        first_line = f.readline().strip()
        # Check if first line looks like column names (contains expected column names)
        if 'intrusion_type' in first_line.lower() or 'duration' in first_line.lower():
            has_header = True
except:
    pass

# Load the dataset
if has_header:
    print("Detected CSV file with headers")
    df = pd.read_csv(dataset_path)
    
    # Map column names to expected format
    # Handle case where CSV uses 'intrusion_type' instead of 'attack_type'
    if 'intrusion_type' in df.columns and 'attack_type' not in df.columns:
        df = df.rename(columns={'intrusion_type': 'attack_type'})
    
    # Get available columns from CSV (excluding difficulty_level if not present)
    available_cols = [col for col in column_names if col in df.columns]
    
    # If difficulty_level doesn't exist, add a dummy column (will be dropped later anyway)
    if 'difficulty_level' not in df.columns:
        df['difficulty_level'] = 0
        available_cols.append('difficulty_level')
    
    # Reorder columns to match expected order
    df = df[column_names] if all(col in df.columns for col in column_names) else df[available_cols]
    
    # If we're missing some columns, fill with defaults
    missing_cols = [col for col in column_names if col not in df.columns]
    if missing_cols:
        print(f"Warning: Missing columns {missing_cols}, filling with defaults")
        for col in missing_cols:
            df[col] = 0
    
    # Reorder to match expected column order
    df = df[column_names]
else:
    print("Detected text file without headers")
    df = pd.read_csv(dataset_path,
                     names=column_names,
                     header=None)

print(f"[STEP 2/6] Dataset loaded successfully")
print(f"  Dataset shape: {df.shape}")
print(f"  Number of samples: {df.shape[0]:,}")
print(f"  Number of features: {df.shape[1]}")
print(f"  Columns: {len(df.columns)} columns")
sys.stdout.flush()

# Display first few rows
df.head(10)

# Basic information
df.info()

# Statistical summary
df.describe()

# Check for missing values
print("Missing values:")
print(df.isnull().sum().sum())
print("\nNo missing values found!" if df.isnull().sum().sum() == 0 else "Missing values detected!")

"""## 3. Exploratory Data Analysis (EDA)"""

# Distribution of attack types
print("Attack Type Distribution:")
print(df['attack_type'].value_counts())
print(f"\nTotal unique attack types: {df['attack_type'].nunique()}")

# Analyze categorical features
categorical_features = ['protocol_type', 'service', 'flag']

print("Categorical Features Analysis:")
for feature in categorical_features:
    print(f"\n{feature.upper()}:")
    print(f"Unique values: {df[feature].nunique()}")
    print(df[feature].value_counts().head(10))

# Correlation analysis for numerical features
numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()
# Remove difficulty_level as it's not a feature
if 'difficulty_level' in numerical_cols:
    numerical_cols.remove('difficulty_level')

print(f"\nNumerical features: {len(numerical_cols)}")

"""## 4. Data Preprocessing"""

print(f"[STEP 3/6] Starting data preprocessing...")
sys.stdout.flush()

# Create a copy for preprocessing
df_processed = df.copy()

# Drop the difficulty_level column as it's not a feature
df_processed = df_processed.drop('difficulty_level', axis=1)

print(f"Shape after dropping difficulty_level: {df_processed.shape}")

# Separate features and target
X = df_processed.drop('attack_type', axis=1)
y = df_processed['attack_type']

print(f"Features shape: {X.shape}")
print(f"Target shape: {y.shape}")
print(f"\nTarget classes: {y.nunique()}")

# Encode categorical features
label_encoders = {}
categorical_columns = ['protocol_type', 'service', 'flag']

for col in categorical_columns:
    le = LabelEncoder()
    X[col] = le.fit_transform(X[col])
    label_encoders[col] = le
    print(f"Encoded {col}: {len(le.classes_)} unique values")

print("\nCategorical encoding completed!")

# Encode target variable
target_encoder = LabelEncoder()
y_encoded = target_encoder.fit_transform(y)

print(f"Number of classes: {len(target_encoder.classes_)}")
print(f"\nClass labels:")
for idx, label in enumerate(target_encoder.classes_[:10]):
    print(f"  {idx}: {label}")
if len(target_encoder.classes_) > 10:
    print(f"  ... and {len(target_encoder.classes_) - 10} more classes")

import numpy as np

# Identify classes with only one sample
class_counts = np.bincount(y_encoded)
single_sample_classes_indices = np.where(class_counts == 1)[0]

# Get the names of these classes for reporting
single_sample_class_names = target_encoder.inverse_transform(single_sample_classes_indices)

print(f"Classes with only 1 sample (will be excluded from stratified split): {single_sample_class_names.tolist()}")

# Create a boolean mask to keep only samples from classes with more than 1 member
mask = np.isin(y_encoded, single_sample_classes_indices, invert=True)

# Filter X and y_encoded
X_filtered = X[mask]
y_filtered = y_encoded[mask]

print(f"Original samples: {len(y_encoded)}")
print(f"Samples after removing single-instance classes: {len(y_filtered)}")

# Split the data
X_train, X_test, y_train, y_test = train_test_split(
    X_filtered, y_filtered, test_size=0.2, random_state=42, stratify=y_filtered
)

print(f"Training set size: {X_train.shape[0]:,} samples")
print(f"Test set size: {X_test.shape[0]:,} samples")
print(f"\nTraining set percentage: {X_train.shape[0]/X.shape[0]*100:.1f}%")
print(f"Test set percentage: {X_test.shape[0]/X.shape[0]*100:.1f}%")

# Feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("[STEP 3/6] Data preprocessing completed!")
print(f"  Scaled training data shape: {X_train_scaled.shape}")
print(f"  Scaled test data shape: {X_test_scaled.shape}")
sys.stdout.flush()

"""## 5. Model Training and Evaluation

### Random Forest Classifier
"""

print(f"[STEP 4/6] Training Random Forest Classifier...")
print("  This may take 2-3 minutes...")
sys.stdout.flush()
rf_model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=20, n_jobs=-1)
rf_model.fit(X_train, y_train)
print("  Random Forest training completed!")
sys.stdout.flush()

# Predictions
y_pred_rf = rf_model.predict(X_test)

# Evaluation
rf_accuracy = accuracy_score(y_test, y_pred_rf)
print(f"\n{'='*60}")
print(f"RANDOM FOREST CLASSIFIER RESULTS")
print(f"{'='*60}")
print(f"Accuracy: {rf_accuracy*100:.2f}%")
sys.stdout.flush()

# Determine the labels and target names for classification reports based on the actual test set
report_labels = np.unique(y_test)
report_target_names = target_encoder.inverse_transform(report_labels)

print(f"Number of classes for classification reports: {len(report_labels)}")
print(f"Classes for reporting: {report_target_names.tolist()}")

"""## 6. Model Results"""

# Save results to JSON for API access
best_model_name = 'Random Forest'
best_accuracy = rf_accuracy * 100
y_pred_best = y_pred_rf
best_model = rf_model

comparison_dict = {
    'models': [
        {'name': 'Random Forest', 'accuracy': float(rf_accuracy), 'accuracy_percent': float(rf_accuracy * 100)}
    ],
    'best_model': best_model_name,
    'best_accuracy': float(rf_accuracy),
    'best_accuracy_percent': float(rf_accuracy * 100)
}

# Save to JSON file for API to read (save in tools directory relative to script location)
script_dir = os.path.dirname(os.path.abspath(__file__))
comparison_json_path = os.path.join(script_dir, 'model_comparison_results.json')
with open(comparison_json_path, 'w') as f:
    json.dump(comparison_dict, f, indent=2)

print(f"\n[STEP 5/6] Model training completed!")
print("="*70)
print(f"MODEL: {best_model_name}")
print(f"Accuracy: {best_accuracy:.2f}%")
print("="*70)
print(f"\n✓ Results saved to: {comparison_json_path}")
sys.stdout.flush()

"""## 7. Detailed Analysis of Best Model"""

print(f"\n{'='*70}")
print(f"MODEL: {best_model_name}")
print(f"Accuracy: {best_accuracy:.2f}%")
print(f"{'='*70}")

# Confusion Matrix for best model
cm = confusion_matrix(y_test, y_pred_best, labels=report_labels)
print(f"\nConfusion Matrix Shape: {cm.shape}")

# Feature importance (if applicable)
if hasattr(best_model, 'feature_importances_'):
    feature_importance = pd.DataFrame({
        'Feature': X.columns,
        'Importance': best_model.feature_importances_
    }).sort_values('Importance', ascending=False)

    print(f"\nTop 15 Most Important Features for {best_model_name}:")
    print(feature_importance.head(15).to_string(index=False))
else:
    print(f"\nFeature importance not available for {best_model_name}")

"""## 8. Per-Class Performance Analysis"""

# Calculate per-class metrics
from sklearn.metrics import precision_recall_fscore_support

precision, recall, f1, support = precision_recall_fscore_support(y_test, y_pred_best, labels=report_labels, zero_division=0)

per_class_metrics = pd.DataFrame({
    'Attack Type': report_target_names,
    'Precision': precision,
    'Recall': recall,
    'F1-Score': f1,
    'Support': support
}).sort_values('Support', ascending=False)

print("\nPer-Class Performance Metrics:")
print(per_class_metrics.to_string(index=False))

"""## 9. Conclusions and Key Findings"""

print("\n" + "="*70)
print("KEY FINDINGS AND CONCLUSIONS")
print("="*70)
print(f"\n1. Dataset Statistics:")
print(f"   - Total samples: {df.shape[0]:,}")
print(f"   - Number of features: {X.shape[1]}")
print(f"   - Number of attack types: {len(target_encoder.classes_)}")
print(f"   - Most common attack: {df['attack_type'].value_counts().index[0]} ({df['attack_type'].value_counts().values[0]:,} samples)")

print(f"\n2. Model Performance:")
print(f"   - Best Model: {best_model_name}")
print(f"   - Best Accuracy: {best_accuracy:.2f}%")
print(f"   - All models achieved accuracy > 90%")

print(f"\n3. Classification Insights:")
print(f"   - The model successfully distinguishes between different attack types")
print(f"   - Most common attack types are well-detected")
print(f"   - Rare attack types may need more training data")

print(f"\n4. Recommendations:")
print(f"   - Use {best_model_name} for production deployment")
print(f"   - Monitor performance on rare attack types")
print(f"   - Consider ensemble methods for improved robustness")
print(f"   - Regularly update the model with new attack patterns")
print("="*70)

"""## 10. Save the Best Model"""

# Save the model locally
import joblib
import os
import sys

# Add parent directory to path to import from src
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

# Ensure models directory exists
models_dir = Path("models")
models_dir.mkdir(exist_ok=True)

model_filename = models_dir / f'kdd_{best_model_name.lower().replace(" ", "_")}_model.pkl'
encoder_filename = models_dir / 'kdd_target_encoder.pkl'
label_encoders_filename = models_dir / 'kdd_label_encoders.pkl'

print(f"[STEP 6/6] Saving model and preprocessing artifacts to models/ folder...")
sys.stdout.flush()

# Save model and preprocessing artifacts
joblib.dump(best_model, model_filename)
joblib.dump(target_encoder, encoder_filename)
joblib.dump(label_encoders, label_encoders_filename)

print(f"  ✓ Model saved: {model_filename}")
print(f"  ✓ Target encoder saved: {encoder_filename}")
print(f"  ✓ Label encoders saved: {label_encoders_filename}")
print(f"\n[STEP 6/6] ✓ COMPLETE - Model and preprocessing artifacts saved successfully!")
print(f"  Model files are ready for use by the consumer service.")
print(f"  Model accuracy: {best_accuracy:.2f}%")
sys.stdout.flush()